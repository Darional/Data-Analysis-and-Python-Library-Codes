'''
Clustering

It's an important tool for Machine Learning and AI. This helps to cluster the information hepling to separate groups of data with certain pattern.
The most common algorithms to cluster are:

K-Means:
        This is the most simple and popular algorithm, selects k initial points (centroids) and asign each poing to the closer centroid,
        then recalculates the centroids as the mean of all points in the cluster and repeats the process until the centroids doesnt's change significantly
    link= https://upload.wikimedia.org/wikipedia/commons/thumb/e/ea/K-means_convergence.gif/500px-K-means_convergence.gif


Hierarchy Clustering:
        This algorithm creates a hierarchy of clusters. Can be "Agglomerative" (Start with each point as a cluster and then combines clusters based
        in a similarity measure). Or "Divisive" (Starts with only 1 cluster that cointains all points then divide recursively in smaller clusters)
    link= https://miro.medium.com/v2/resize:fit:1400/0*dBKZS5-GpteqWOdk.gif


DBSCAN ( Density-Based Spatial Clustering of Applications with Noise):
        Is a density-based algorithm that defines the clusters as high-density separated by low-density areas. Doesn't require to specify the numbers of
        clusters before and it can find clusters in an arbitrary way, this is an advantage over K-Means 
    link= https://iq.opengenus.org/content/images/2018/07/1-tc8UF-h0nQqUfLC8-0uInQ.gif


Expectation-Maximization (EM) Clustering using Gaussian Mixture Models (GMM)
        A disadvantage of K-Means is the naive use of the mean to the center cluster (https://dm.cs.tu-dortmund.de/mlbits/cluster-kmeans-limitations/mouse-ssq-k5.svg)
        With GMM, we asume that the data are distribuited like a Gaussian Model, this is less restrictive than say the clusters are circles. So we can use the 
        standard deviation and the mean, using these two parameters we can describe the shape of the clusters. That means, each gaussian distribution has it's
        own a unique cluster. To find the parameters of the gaussian distribution for each cluster, we will use an optimization algorithm called
        Expectation Maximization (EM)   

    
Spectral Clustering: 
        Use the eigenvectors of a similarity matrix to reduce the dimension of the data before apply other cluster algorithm. It's useful when the structure
        of the cluster is more complex and not only spherical


Model Clustering-Based:
        This asumes that data is generated by a group of internal models. One of the most popular example is the GMM that asumes the data is generated by a
        mixture of multiple Gaussian Distributions


Neural Network Clustering-Based: 
        Some neural network as "Self-Organizing Map" (SOM) or "Autoencoder" can be used to do clustering. These models can capture complex structures and 
        non-lineal structures of the data.


Optimization Clustering-Based:
        The cluster is formulated as an optimization problem that is solved to find the best divition of the data.
'''

import pandas as pd
import numpy as np
from sklearn.cluster import KMeans
from sklearn import preprocessing
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.pipeline import make_pipeline
import matplotlib.pyplot as plt

import plotly.express as pxp
import plotly.graph_objs as gph
import missingno as msno

import seaborn as sns
from sklearn import metrics
from sklearn.datasets import make_blobs
from sklearn.preprocessing import StandardScaler

from sklearn.cluster import KMeans
from scipy.cluster.hierarchy import dendrogram, linkage
from sklearn.cluster import AgglomerativeClustering
from sklearn.cluster import DBSCAN
from sklearn.metrics import silhouette_score



data = pd.read_csv('./Customers.csv')
print(data.head(),'\n')
print(data.describe().T,'\n')
print(data.info(),'\n')
print(data.isnull().sum(),'\n')

# Vissual Way to observe nulls data, the height is the non-null data.
msno.bar(data, fontsize=12)
plt.show()

# Deleting non required clomun CustomerID
data.drop("CustomerID", axis=1, inplace=True)

# Using Exploratory Data Analysis
sns.pairplot(data = data, hue='Gender', palette='RdYlBu')
plt.show()

# Show the count of each column
plt.figure(figsize = (15, 4))
plotnum = 1

for cols in ['Age', 'AnnualIncome', 'SpendingScore']:
    if plotnum <= 3:
        ax = plt.subplot(1, 3, plotnum)
        sns.histplot(data[cols], ax=ax, kde=True, color='y')

    plotnum += 1

plt.tight_layout()
plt.show()



'''
Grouping by age intervals.
'''
# Defining the age intervals
bins = [18, 25, 35, 45, 55, data['Age'].max()]
in_bins = [10, 30, 50, 70, 90, 110, data['AnnualIncome'].max()]

# Defining the names of the groups
age_group_names = ['18-25', '26-35', '36-45', '46-55', '55+']
in_group_names = ['10-30', '31-50', '51-70', '71-90', '91-110', '111+']

# Creating the new column on the DataFrame that represents the age group
data['AgeGroup'] = pd.cut(data['Age'], bins, labels=age_group_names, right=False)
# Creating new column on the DataFrame that represents the income Group
data['IncomeGroup'] = pd.cut(data['AnnualIncome'], in_bins, labels=in_group_names, right=False)

# Let's group by age and income
age_group = data.groupby('AgeGroup')
print(age_group.size(),'\n')
income_group = data.groupby('IncomeGroup')
print(income_group.size(),'\n')

# Ploting the income and age group
sns.countplot(x='AgeGroup', data=data, order= age_group_names)
plt.xlabel('Age groups')
plt.ylabel('Count')
plt.title('Age Distribution')
plt.show()

sns.countplot(x='IncomeGroup', data=data, order= in_group_names)
plt.xlabel('Income groups')
plt.ylabel('Count')
plt.title('Income Distribution')
plt.show()


# Doing Scatterplot of the Annual Income and Age Group vs Spending Score
sns.scatterplot(x='AnnualIncome', y='SpendingScore', data=data)
plt.xlabel('Annual Income')
plt.ylabel('Spending Score')
plt.title('Relation between Annual Income and Spending Score')
plt.show()
sns.scatterplot(x='Age', y='SpendingScore', data=data)
plt.xlabel('Age')
plt.ylabel('Spending Score')
plt.title('Relation between Age and Spending Score')
plt.show()


'''
Finding the K (Numbers of Clusters)

We cannot use an aleatory K, so we are using the "Elbow Method" and "Silhouette Coefficient" to maximize the number of the clusters and
limitating the situations where each point is it's own centroid of the cluster.
'''

x_input = data.loc[:,['Age', 'SpendingScore']].values
wcss = []  # Within-Cluster Sum of Sqares, this is the sum of the cuadratic error (less the value, better fits)
for k in range(2,12):  # Training the model for differents K
    label = []
    k_means = KMeans(n_clusters=k, init='k-means++', n_init=10)  # n_clusters define the number of the clusters to use, init='k-mean++' for a better initialization of the centroids, n_init=10, means for every K, K-Means will be execute 10 different times (avoid the local minimums) for better results
    k_means.fit(x_input)  # Training the model -> calculating the centroids and asign every point to a cluster.
    label = k_means.predict(x_input)
    print(f'Silhouette Score(n={k}): {silhouette_score(x_input, label):3.1f}')  # closer to 1, nice separation, closer to 0.5 cluster well defined, acceptable.
    wcss.append(k_means.inertia_)  # Appends the associate wcss 

plt.figure(figsize=(8,4))

plt.plot(range(2,12), wcss, linewidth=2, marker='8')
plt.title("Elbow Method")
plt.xlabel("K")
plt.ylabel("WCSS")
plt.show()


'''
The Silhouette Score is similar (‚âà0.5) for k=2,3,4,5, but ùëò=4 is selected as the best option since it provides the clearest separation between
clusters without unnecessarily increasing the model's complexity
'''

# Graphicating the cluster
k_means = KMeans(n_clusters=4, init='k-means++', n_init=10) 
k_means.fit(x_input)
label = k_means.predict(x_input)
print(f' Silhouette Score(n=4): {silhouette_score(x_input, label):3.1f}')

plt.figure(figsize = (10, 6))
plt.scatter(x_input[:, 0],
             x_input[:, 1],
             c=k_means.labels_*7,
             s=20)
plt.scatter(k_means.cluster_centers_[:, 0],
             k_means.cluster_centers_[:,1],
             color='red',
             s=50)
plt.title('Clusters de clientes', fontsize = 10)
plt.xlabel('Edad')
plt.ylabel('Puntuaci√≥n de gasto (Spending Score)')
plt.show()