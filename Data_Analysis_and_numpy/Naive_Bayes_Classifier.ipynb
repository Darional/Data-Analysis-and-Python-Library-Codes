{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b80ae34",
   "metadata": {},
   "source": [
    "# The Principle of Naive Bayes\n",
    "\n",
    "The Naive Bayes algorithms rely on the Bayes' theorem. Let's recall it quickly. This theorem calculates the probability of an event based on prior knowledge of potentially related events. It is represented mathematically as:\n",
    "\n",
    "$P(A|B) = \\dfrac{P(B|A)\\cdot P(A)}{P(B)}$\n",
    "\n",
    "Where $P(A|B)$ is the posterior probability of class ($A$) given predictor ($B$). It's what we are trying to calculate. $P(B|A)$ is the likelihood, which is the probability of the predictor given a class. $P(B)$ is the marginal probability of predictor, and $P(A)$ is the prior probability of the class. This formula forms the backbone of the Naive Bayes classifier.  \n",
    "\n",
    "The term 'naive' refers to the assumption that all variables in a dataset are independent of each other, which may not always be the case in real-life data. Nonetheless, it still offers robust performance and can be easily implemented.\n",
    "\n",
    "# Naive Bayes Classifier: Derivation and Example\n",
    "\n",
    "## 1. Bayes Theorem in Classification\n",
    "In **Naive Bayes Classification**, we want to compute the posterior probability:\n",
    "\n",
    "$\n",
    "P(Y=1 \\mid X_1=x_1, X_2=x_2, ..., X_n=x_n)\n",
    "$\n",
    "\n",
    "Using Bayes' theorem:\n",
    "\n",
    "$\n",
    "P(Y \\mid X) = \\frac{P(X \\mid Y) \\, P(Y)}{P(X)}\n",
    "$\n",
    "\n",
    "- \\(P(Y)\\): **Prior** — probability of the class before seeing data.\n",
    "- \\(P(X \\mid Y)\\): **Likelihood** — probability of features given the class.\n",
    "- \\(P(X)\\): **Evidence** (denominator, same for all classes).\n",
    "- \\(P(Y \\mid X)\\): **Posterior** — probability of the class given the features.\n",
    "\n",
    "Since the denominator \\(P(X)\\) is constant across classes, the classifier maximizes:\n",
    "\n",
    "$\n",
    "P(Y, X) = P(X \\mid Y) \\, P(Y)\n",
    "$\n",
    "\n",
    "This is the foundation of **Naive Bayes Classification**.\n",
    "\n",
    "\n",
    "## 2. Example Dataset\n",
    "\n",
    "| Temperature | Humidity | Weather |\n",
    "|-------------|----------|---------|\n",
    "| Hot         | High     | Sunny   |\n",
    "| Hot         | High     | Sunny   |\n",
    "| Cold        | Normal   | Snowy   |\n",
    "| Hot         | Normal   | Rainy   |\n",
    "| Cold        | High     | Snowy   |\n",
    "| Cold        | Normal   | Snowy   |\n",
    "| Cold        | Normal   | Sunny   |\n",
    "\n",
    "Classes for `Weather`: **Sunny (3)**, **Rainy (1)**, **Snowy (3)**.  \n",
    "Total = 7 instances.\n",
    "\n",
    "\n",
    "### 3. Prior Probabilities\n",
    "\n",
    "* $ P(Sunny) = \\frac{3}{7} \\approx 0.43$\n",
    "\n",
    "* $P(Rainy) = \\frac{1}{7} \\approx 0.14$\n",
    "\n",
    "* $P(Snowy) = \\frac{3}{7} \\approx 0.43$\n",
    "\n",
    "### 4. Likelihoods\n",
    "\n",
    "We compute conditional probabilities of features given the class.\n",
    "\n",
    "#### For **Sunny** (3 instances):\n",
    "* $P(Hot \\mid Sunny) = \\frac{2}{3} \\approx 0.67$\n",
    "\n",
    "* $P(Cold \\mid Sunny) = \\frac{1}{3} \\approx 0.33$\n",
    "* $P(High \\mid Sunny) = \\frac{2}{3} \\approx 0.67$\n",
    "\n",
    "* $P(Normal \\mid Sunny) = \\frac{1}{3} \\approx 0.33$\n",
    "\n",
    "\n",
    "\n",
    "#### For **Rainy** (1 instance):\n",
    "* $P(Hot \\mid Rainy) = 1.00$\n",
    "\n",
    "* $P(Cold \\mid Rainy) = 0.00$\n",
    "\n",
    "* $P(High \\mid Rainy) = 0.00$\n",
    "\n",
    "* $P(Normal \\mid Rainy) = 1.00$\n",
    "\n",
    "\n",
    "\n",
    "#### For **Snowy** (3 instances):\n",
    "* $ P(Hot \\mid Snowy) = 0.00$\n",
    "\n",
    "* $P(Cold \\mid Snowy) = 1.00$\n",
    "\n",
    "* $P(High \\mid Snowy) = \\frac{1}{3} \\approx 0.33$\n",
    "\n",
    "* $P(Normal \\mid Snowy) = \\frac{2}{3} \\approx 0.67$\n",
    "\n",
    "\n",
    "\n",
    "### 5. Summary\n",
    "- **Prior probabilities** represent how frequent each weather condition is overall.  \n",
    "- **Likelihoods** represent how features (Temperature, Humidity) distribute **within each class**.  \n",
    "- Together, they let us compute the posterior for a new case and classify it by picking the class with the highest posterior probability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea1ab1a",
   "metadata": {},
   "source": [
    "# Implementing Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "337f75f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def calculate_prior_probabilities(y):\n",
    "    #  Calculate prior probabilities for each class\n",
    "    return y.value_counts(normalize=True)\n",
    "\n",
    "\n",
    "def calculate_likehoods(X, y):\n",
    "    likehoods = {}\n",
    "    for column in X.columns:\n",
    "        likehoods[column] = {}\n",
    "        for class_ in y.unique():\n",
    "            # Filter feature column data for each class\n",
    "            class_data = X[y == class_][column]\n",
    "            counts = class_data.value_counts()\n",
    "            total_count = len(class_data) # Total count of instances for current class\n",
    "            likehoods[column][class_] = counts / total_count\n",
    "    return likehoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6e7b336",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_bayes_classifier(X_test, priors, likelihoods):\n",
    "    predictions = []\n",
    "    for _, data_point in X_test.iterrows():\n",
    "        class_probabilities = {}\n",
    "        for class_ in priors.index:\n",
    "            class_probabilities[class_] = priors[class_]\n",
    "            for feature in X_test.columns:\n",
    "                # Use .get to safely retrieve probability and get a default of 1/total to handle unseen values\n",
    "                feature_probs = likelihoods[feature][class_]\n",
    "                class_probabilities[class_] *= feature_probs.get(data_point[feature], 1 / (len(feature_probs) + 1))\n",
    "\n",
    "        # Predict class with maximum posterior probability\n",
    "        predictions.append(max(class_probabilities, key=class_probabilities.get))\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9472d5c",
   "metadata": {},
   "source": [
    "### Understanding and Handling Data Issues in Naive Bayes\n",
    "A recurring challenge in Naive Bayes is the handling of zero probabilities, i.e., when a category does not appear in the training data for a given class, resulting in a zero probability for that category. A known fix for this problem is applying Laplace or Add-1 smoothing, which adds a '1' to each category count to circumvent zero probabilities.\n",
    "\n",
    "You can integrate Laplace smoothing into the calculate_likelihoods function as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "404dcba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_likelihoods_with_smoothing(X, y):\n",
    "    likelihoods = {}\n",
    "    for column in X.columns:\n",
    "        likelihoods[column] = {}\n",
    "        for class_ in y.unique():\n",
    "            # Calculate normalized counts with smoothing\n",
    "            class_data = X[y == class_][column]\n",
    "            counts = class_data.value_counts()\n",
    "            total_count = len(class_data) + len(X[column].unique())  # total count with smoothing\n",
    "            likelihoods[column][class_] = (counts + 1) / total_count  # add-1 smoothing\n",
    "    return likelihoods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce5203f",
   "metadata": {},
   "source": [
    "The numerator is increased by 1 and the denominator by the count of unique categories to accommodate the added 1's.\n",
    "\n",
    "## Using Naive Bayes Classifier\n",
    "Here is a short example of predicting weather with our classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c763ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Weather:  Snowy\n"
     ]
    }
   ],
   "source": [
    "data = {\n",
    "    'Temperature': ['Hot', 'Hot', 'Cold', 'Hot', 'Cold', 'Cold', 'Cold'],\n",
    "    'Humidity': ['High', 'High', 'Normal', 'Normal', 'High', 'Normal', 'Normal'],\n",
    "    'Weather': ['Sunny', 'Sunny', 'Snowy', 'Rainy', 'Snowy', 'Snowy', 'Sunny']\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Split features and labels\n",
    "X = df[['Temperature', 'Humidity']]\n",
    "y = df['Weather']\n",
    "\n",
    "# Calculate prior probabilities\n",
    "priors = calculate_prior_probabilities(y)\n",
    "\n",
    "# Calculate likelihoods with smoothing\n",
    "likelihoods = calculate_likelihoods_with_smoothing(X, y)\n",
    "\n",
    "# New observation\n",
    "X_test = pd.DataFrame([{'Temperature': 'Cold', 'Humidity': 'Normal'}])\n",
    "\n",
    "# Make prediction\n",
    "prediction = naive_bayes_classifier(X_test, priors, likelihoods)\n",
    "print(\"Predicted Weather: \", prediction[0])  # Output: Predicted Weather:  Snowy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataScience",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
